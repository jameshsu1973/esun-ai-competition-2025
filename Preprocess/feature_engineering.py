"""ç‰¹å¾µå·¥ç¨‹æ¨¡çµ„ - å¾äº¤æ˜“è³‡æ–™ä¸­æå– 62 ç¶­ç‰¹å¾µå‘é‡ã€‚

æ­¤æ¨¡çµ„è² è²¬å¾åŸå§‹äº¤æ˜“è³‡æ–™ä¸­æå–è±å¯Œçš„ç‰¹å¾µï¼Œç”¨æ–¼è©æ¬ºåµæ¸¬æ¨¡å‹è¨“ç·´ã€‚
ç‰¹å¾µæ¶µè“‹åŸºæœ¬çµ±è¨ˆã€é‡‘é¡ç‰¹å¾µã€æ™‚é–“ç‰¹å¾µã€å°æ‰‹æ–¹ç‰¹å¾µã€äº¤æ˜“é¡å‹èˆ‡é¢¨éšªæŒ‡æ¨™ã€‚

Classes:
    FeatureEngineer: ç‰¹å¾µå·¥ç¨‹å™¨ï¼Œæä¾›è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†ç‰¹å¾µæå–åŠŸèƒ½ã€‚

Features (62 ç¶­):
    - åŸºæœ¬äº¤æ˜“çµ±è¨ˆ (5 å€‹): äº¤æ˜“æ¬¡æ•¸ã€è½‰å…¥è½‰å‡ºæ¯”ä¾‹ç­‰
    - é‡‘é¡ç‰¹å¾µ (23 å€‹): ç¸½é¡ã€å‡å€¼ã€æ¨™æº–å·®ã€åˆ†ä½æ•¸ç­‰
    - æ™‚é–“ç‰¹å¾µ (13 å€‹): æ—¥æœŸç¯„åœã€æ´»èºå¤©æ•¸ã€å¤œé–“äº¤æ˜“æ¯”ä¾‹ç­‰
    - å°æ‰‹æ–¹ç‰¹å¾µ (4 å€‹): äº¤æ˜“å°è±¡å¤šæ¨£æ€§ã€é›†ä¸­åº¦ç­‰
    - äº¤æ˜“é¡å‹ç‰¹å¾µ (11 å€‹): è·¨è¡Œäº¤æ˜“ã€å¤–å¹£äº¤æ˜“ã€è‡ªè½‰äº¤æ˜“ç­‰
    - é¢¨éšªæŒ‡æ¨™ (14 å€‹): å¤§é¡äº¤æ˜“æ¯”ä¾‹ã€é›†ä¸­åº¦ã€æ™‚é–“ç†µç­‰

Example:
    >>> from Preprocess.feature_engineering import FeatureEngineer
    >>> engineer = FeatureEngineer(df_txn)
    >>> df_train = engineer.extract_train_features(df_alert, use_cache=True)
    >>> print(f"ç‰¹å¾µç¶­åº¦: {len(df_train.columns) - 2}")  # æ‰£é™¤ acct å’Œ label
    62
"""

import time
import joblib
import os
import numpy as np
import pandas as pd
from Config.config import Config

class FeatureEngineer:
    """ç‰¹å¾µå·¥ç¨‹å™¨ - å¾äº¤æ˜“è³‡æ–™ä¸­æå– 62 ç¶­ç‰¹å¾µã€‚
    
    æ­¤é¡åˆ¥å¯¦ä½œå®Œæ•´çš„ç‰¹å¾µæå–æµç¨‹ï¼ŒåŒ…å«ï¼š
    - High Activity 10x æ¡æ¨£ç­–ç•¥ï¼ˆé‡å°è¨“ç·´é›†ï¼‰
    - æ‰¹æ¬¡æå–ç‰¹å¾µä¸¦é¡¯ç¤ºé€²åº¦
    - å¿«å–æ©Ÿåˆ¶ä»¥åŠ é€Ÿé‡è¤‡åŸ·è¡Œ
    - è™•ç†ç„¡äº¤æ˜“å¸³æˆ¶çš„é›¶å¡«å……
    
    Attributes:
        df_txn (pd.DataFrame): äº¤æ˜“è³‡æ–™ï¼Œéœ€åŒ…å« from_acct, to_acct, txn_amt ç­‰æ¬„ä½ã€‚
    
    Example:
        >>> engineer = FeatureEngineer(df_txn)
        >>> # æå–è¨“ç·´é›†ç‰¹å¾µï¼ˆå«æ¡æ¨£ï¼‰
        >>> df_train = engineer.extract_train_features(df_alert)
        >>> # æå–æ¸¬è©¦é›†ç‰¹å¾µ
        >>> df_test = engineer.extract_test_features(df_predict)
    """
    
    def __init__(self, df_txn):
        """åˆå§‹åŒ–ç‰¹å¾µå·¥ç¨‹å™¨ã€‚
        
        Args:
            df_txn (pd.DataFrame): äº¤æ˜“è³‡æ–™ï¼Œéœ€åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
                - from_acct: è½‰å‡ºå¸³æˆ¶
                - to_acct: è½‰å…¥å¸³æˆ¶
                - txn_amt: äº¤æ˜“é‡‘é¡ï¼ˆå·²è½‰æ›ç‚º TWDï¼‰
                - txn_date: äº¤æ˜“æ—¥æœŸ
                - txn_hour: äº¤æ˜“å°æ™‚
                - is_self_txn: æ˜¯å¦ç‚ºè‡ªè½‰äº¤æ˜“
                - channel_type: äº¤æ˜“é€šè·¯
                - currency_type: å¹£åˆ¥
                - from_acct_type: è½‰å‡ºå¸³æˆ¶é¡å‹ï¼ˆ1=ç‰å±±, 2=ä»–è¡Œï¼‰
                - to_acct_type: è½‰å…¥å¸³æˆ¶é¡å‹ï¼ˆ1=ç‰å±±, 2=ä»–è¡Œï¼‰
        """
        self.df_txn = df_txn
    
    def extract_train_features(self, df_alert, use_cache=True):
        """æå–è¨“ç·´é›†ç‰¹å¾µï¼ˆå« High Activity æ¡æ¨£ç­–ç•¥ï¼‰ã€‚
        
        æ­¤æ–¹æ³•å¯¦ä½œ High Activity 10x æ¡æ¨£ç­–ç•¥ï¼š
        1. è­˜åˆ¥æ‰€æœ‰è­¦ç¤ºå¸³æˆ¶ä½œç‚ºæ­£æ¨£æœ¬
        2. è¨ˆç®—æ‰€æœ‰éè­¦ç¤ºå¸³æˆ¶çš„äº¤æ˜“é‡
        3. å¾ Top 30% é«˜æ´»èºå¸³æˆ¶ä¸­éš¨æ©Ÿæ¡æ¨£ 10x è² æ¨£æœ¬
        4. æå–æ‰€æœ‰é¸å®šå¸³æˆ¶çš„ç‰¹å¾µ
        5. æ·»åŠ æ¨™ç±¤æ¬„ä½ï¼ˆlabel: 1=è­¦ç¤º, 0=æ­£å¸¸ï¼‰
        
        Args:
            df_alert (pd.DataFrame): è­¦ç¤ºå¸³æˆ¶æ¸…å–®ï¼Œéœ€åŒ…å« acct æ¬„ä½ã€‚
            use_cache (bool, optional): æ˜¯å¦ä½¿ç”¨å¿«å–ã€‚
                è‹¥ç‚º True ä¸”å¿«å–æª”æ¡ˆå­˜åœ¨ï¼Œå‰‡ç›´æ¥è¼‰å…¥å¿«å–ã€‚
                é è¨­ç‚º Trueã€‚
        
        Returns:
            pd.DataFrame: åŒ…å«ç‰¹å¾µå’Œæ¨™ç±¤çš„è¨“ç·´è³‡æ–™ï¼Œæ¬„ä½åŒ…æ‹¬ï¼š
                - acct: å¸³æˆ¶ ID
                - 62 å€‹ç‰¹å¾µæ¬„ä½ï¼ˆè¦‹æ¨¡çµ„ docstringï¼‰
                - label: æ¨™ç±¤ï¼ˆ1=è­¦ç¤º, 0=æ­£å¸¸ï¼‰
        
        Example:
            >>> engineer = FeatureEngineer(df_txn)
            >>> df_train = engineer.extract_train_features(df_alert, use_cache=True)
            >>> print(f"è¨“ç·´æ¨£æœ¬æ•¸: {len(df_train)}")
            >>> print(f"è­¦ç¤ºæ¯”ä¾‹: {df_train['label'].mean():.2%}")
        """
        print("="*70)
        print(f"ğŸ“Š [ç‰¹å¾µå·¥ç¨‹] æº–å‚™è¨“ç·´è³‡æ–™ - ç­–ç•¥: {Config.SAMPLING_STRATEGY}")
        
        # æª¢æŸ¥å¿«å–
        if use_cache and os.path.exists(Config.TRAIN_FEATURE_CACHE):
            print(f"âœ“ è¼‰å…¥è¨“ç·´ç‰¹å¾µå¿«å–: {Config.TRAIN_FEATURE_CACHE}")
            cache_data = joblib.load(Config.TRAIN_FEATURE_CACHE)
            df_features = cache_data['df_train']
            print(f"âœ“ ç‰¹å¾µæ•¸: {len(df_features.columns)-2}, æ¨£æœ¬æ•¸: {len(df_features):,}")
            print(f"âœ“ è­¦ç¤ºæ¯”ä¾‹: {df_features['label'].mean():.4%}")
            return df_features
        
        alert_accts = set(df_alert['acct'].unique())
        print(f"è­¦ç¤ºå¸³æˆ¶æ•¸: {len(alert_accts)}")
        
        # è¨ˆç®—æ‰€æœ‰ç‰å±±å¸³æˆ¶çš„äº¤æ˜“é‡
        esun_out = self.df_txn[self.df_txn['from_acct_type'] == 1].groupby('from_acct').size()
        esun_in = self.df_txn[self.df_txn['to_acct_type'] == 1].groupby('to_acct').size()
        txn_counts = pd.concat([esun_out, esun_in]).groupby(level=0).sum().sort_values(ascending=False)
        
        # æ’é™¤è­¦ç¤ºå¸³æˆ¶
        non_alert_txn_counts = txn_counts[~txn_counts.index.isin(alert_accts)]
        
        # High Activity æ¡æ¨£
        top_n = int(len(non_alert_txn_counts) * Config.TOP_PERCENTILE)
        high_activity_accts = non_alert_txn_counts.head(top_n).index.tolist()
        
        sample_size = min(len(high_activity_accts), len(alert_accts) * Config.NEGATIVE_MULTIPLIER)
        np.random.seed(Config.RANDOM_SEED)
        sampled_non_alert = np.random.choice(high_activity_accts, size=sample_size, replace=False)
        
        print(f"å¯é¸é«˜æ´»èºå¸³æˆ¶ (Top {Config.TOP_PERCENTILE*100:.0f}%): {len(high_activity_accts):,}")
        print(f"å¯¦éš›æ¡æ¨£è² æ¨£æœ¬: {len(sampled_non_alert):,}")
        
        train_accts = list(alert_accts) + list(sampled_non_alert)
        print(f"è¨“ç·´å¸³æˆ¶ç¸½æ•¸: {len(train_accts):,} (è­¦ç¤º:{len(alert_accts)} + æ¡æ¨£:{len(sampled_non_alert)})")
        
        # æå–ç‰¹å¾µ
        df_features = self._extract_features_batch(train_accts, stage_name="è¨“ç·´ç‰¹å¾µæå–")
        df_features['label'] = df_features['acct'].apply(lambda x: 1 if x in alert_accts else 0)
        
        print(f"âœ“ ç‰¹å¾µæ•¸: {len(df_features.columns)-2}, è­¦ç¤ºæ¯”ä¾‹: {df_features['label'].mean():.4%}")
        
        # å¿«å–
        if use_cache:
            joblib.dump({'df_train': df_features}, Config.TRAIN_FEATURE_CACHE, compress=3)
            print(f"âœ“ å·²å¿«å–è‡³ {Config.TRAIN_FEATURE_CACHE}")
        
        return df_features
    
    def extract_test_features(self, df_predict, use_cache=True):
        """æå–æ¸¬è©¦é›†ç‰¹å¾µã€‚
        
        ç‚ºæ‰€æœ‰å¾…é æ¸¬å¸³æˆ¶æå–ç›¸åŒçš„ 62 ç¶­ç‰¹å¾µå‘é‡ã€‚
        
        Args:
            df_predict (pd.DataFrame): å¾…é æ¸¬å¸³æˆ¶æ¸…å–®ï¼Œéœ€åŒ…å« acct æ¬„ä½ã€‚
            use_cache (bool, optional): æ˜¯å¦ä½¿ç”¨å¿«å–ã€‚é è¨­ç‚º Trueã€‚
        
        Returns:
            pd.DataFrame: åŒ…å«ç‰¹å¾µçš„æ¸¬è©¦è³‡æ–™ï¼Œæ¬„ä½åŒ…æ‹¬ï¼š
                - acct: å¸³æˆ¶ ID
                - 62 å€‹ç‰¹å¾µæ¬„ä½
        
        Example:
            >>> engineer = FeatureEngineer(df_txn)
            >>> df_test = engineer.extract_test_features(df_predict)
            >>> print(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(df_test)}")
        """
        print("="*70)
        print("ğŸ¯ [ç‰¹å¾µå·¥ç¨‹] æº–å‚™æ¸¬è©¦è³‡æ–™")
        
        # æª¢æŸ¥å¿«å–
        if use_cache and os.path.exists(Config.TEST_FEATURE_CACHE):
            print(f"âœ“ è¼‰å…¥æ¸¬è©¦ç‰¹å¾µå¿«å–: {Config.TEST_FEATURE_CACHE}")
            df_features = joblib.load(Config.TEST_FEATURE_CACHE)
            print(f"âœ“ ç‰¹å¾µæ•¸: {len(df_features.columns)-1}, æ¨£æœ¬æ•¸: {len(df_features):,}")
            return df_features
        
        test_accts = df_predict['acct'].tolist()
        print(f"æ¸¬è©¦å¸³æˆ¶æ•¸: {len(test_accts)}")
        
        # æå–ç‰¹å¾µ
        df_features = self._extract_features_batch(test_accts, stage_name="æ¸¬è©¦ç‰¹å¾µæå–")
        
        # å¿«å–
        if use_cache:
            joblib.dump(df_features, Config.TEST_FEATURE_CACHE, compress=3)
            print(f"âœ“ å·²å¿«å–è‡³ {Config.TEST_FEATURE_CACHE}")
        
        return df_features
    
    def _extract_features_batch(self, account_list, stage_name="ç‰¹å¾µæå–"):
        """æ‰¹æ¬¡æå–å¸³æˆ¶ç‰¹å¾µä¸¦é¡¯ç¤ºé€²åº¦ã€‚
        
        æ­¤æ–¹æ³•æ‰¹æ¬¡è™•ç†å¸³æˆ¶æ¸…å–®ï¼Œç‚ºæ¯å€‹å¸³æˆ¶æå–ç‰¹å¾µï¼Œ
        ä¸¦åœ¨è™•ç†éç¨‹ä¸­é¡¯ç¤ºé€²åº¦è³‡è¨Šï¼ˆæ¯ 1000 å€‹å¸³æˆ¶æ›´æ–°ä¸€æ¬¡ï¼‰ã€‚
        
        Args:
            account_list (list): å¸³æˆ¶ ID æ¸…å–®ã€‚
            stage_name (str, optional): éšæ®µåç¨±ï¼Œç”¨æ–¼æ—¥èªŒè¼¸å‡ºã€‚
                é è¨­ç‚º "ç‰¹å¾µæå–"ã€‚
        
        Returns:
            pd.DataFrame: åŒ…å«æ‰€æœ‰å¸³æˆ¶ç‰¹å¾µçš„ DataFrameã€‚
        
        Note:
            - æ¯ 1000 å€‹å¸³æˆ¶é¡¯ç¤ºä¸€æ¬¡é€²åº¦
            - é¡¯ç¤ºè™•ç†é€Ÿåº¦ï¼ˆaccts/sï¼‰å’Œé ä¼°å‰©é¤˜æ™‚é–“
            - å°æ–¼ç„¡äº¤æ˜“å¸³æˆ¶ï¼Œä½¿ç”¨ _get_empty_features() å¡«å……é›¶å€¼
        """
        print(f"\nğŸ”§ [{stage_name}] æå– {len(account_list):,} å€‹å¸³æˆ¶çš„ç‰¹å¾µ...")
        start_time = time.time()
        
        features_list = []
        for i, acct in enumerate(account_list):
            if (i+1) % 1000 == 0:
                elapsed = time.time() - start_time
                speed = (i+1) / elapsed
                remaining = (len(account_list) - i - 1) / speed
                print(f"  é€²åº¦: {i+1:>6}/{len(account_list)} ({i/len(account_list)*100:>5.1f}%) | "
                      f"é€Ÿåº¦: {speed:.1f} accts/s | å‰©é¤˜: {remaining/60:.1f} min")
            
            out_txns = self.df_txn[self.df_txn['from_acct'] == acct]
            in_txns = self.df_txn[self.df_txn['to_acct'] == acct]
            all_txns = pd.concat([out_txns, in_txns])
            
            features = {'acct': acct}
            
            if len(all_txns) == 0:
                features.update(self._get_empty_features())
            else:
                features.update(self._extract_account_features(out_txns, in_txns, all_txns))
            
            features_list.append(features)
        
        elapsed = time.time() - start_time
        print(f"âœ… ç‰¹å¾µæå–å®Œæˆ | è€—æ™‚: {elapsed:.1f}s | é€Ÿåº¦: {len(account_list)/elapsed:.1f} accts/s")
        return pd.DataFrame(features_list)
    
    def _extract_account_features(self, out_txns, in_txns, all_txns):
        """æå–å–®å€‹å¸³æˆ¶çš„ 62 ç¶­ç‰¹å¾µå‘é‡ã€‚
        
        æ­¤æ–¹æ³•ç‚ºå–®ä¸€å¸³æˆ¶è¨ˆç®—æ‰€æœ‰ç‰¹å¾µï¼ŒåŒ…å«ï¼š
        
        1. åŸºæœ¬äº¤æ˜“çµ±è¨ˆ (5 å€‹):
           - out_count, in_count, total_count: è½‰å‡º/è½‰å…¥/ç¸½äº¤æ˜“æ¬¡æ•¸
           - txn_ratio: è½‰å‡ºè½‰å…¥æ¯”ä¾‹
           - in_out_diff: è½‰å‡ºè½‰å…¥æ¬¡æ•¸å·®ç•°
        
        2. é‡‘é¡ç‰¹å¾µ (23 å€‹):
           - out_amt_*: è½‰å‡ºé‡‘é¡çµ±è¨ˆï¼ˆsum, mean, std, max, min, median, cv, q25, q75, iqrï¼‰
           - in_amt_*: è½‰å…¥é‡‘é¡çµ±è¨ˆï¼ˆsum, mean, medianï¼‰
           - amt_in_out_ratio: è½‰å…¥è½‰å‡ºé‡‘é¡æ¯”ä¾‹
           - amt_balance: è½‰å…¥è½‰å‡ºé‡‘é¡å·®
        
        3. æ™‚é–“ç‰¹å¾µ (13 å€‹):
           - date_range: äº¤æ˜“æ—¥æœŸç¯„åœ
           - txn_velocity: äº¤æ˜“é€Ÿåº¦ï¼ˆæ¬¡æ•¸/å¤©ï¼‰
           - active_days, active_day_ratio: æ´»èºå¤©æ•¸èˆ‡æ¯”ä¾‹
           - night_txn_count, night_txn_ratio: å¤œé–“äº¤æ˜“çµ±è¨ˆ
           - work_hour_count, work_hour_ratio: å·¥ä½œæ™‚æ®µäº¤æ˜“çµ±è¨ˆ
           - first_week_ratio, last_week_ratio: é¦–é€±æœ«é€±äº¤æ˜“æ¯”ä¾‹
           - first_last_week_diff: é¦–æœ«é€±å·®ç•°
           - burst_score, hour_entropy: çˆ†ç™¼åˆ†æ•¸èˆ‡æ™‚é–“ç†µ
        
        4. å°æ‰‹æ–¹ç‰¹å¾µ (4 å€‹):
           - unique_from, unique_to: å”¯ä¸€è½‰å…¥/è½‰å‡ºå°è±¡æ•¸
           - unique_counterparties: å”¯ä¸€å°æ‰‹æ–¹ç¸½æ•¸
           - counterparty_diversity: å°æ‰‹æ–¹å¤šæ¨£æ€§
        
        5. äº¤æ˜“é¡å‹ç‰¹å¾µ (11 å€‹):
           - self_txn_count, self_txn_ratio: è‡ªè½‰äº¤æ˜“çµ±è¨ˆ
           - unique_channels, channel_diversity: é€šè·¯å¤šæ¨£æ€§
           - foreign_currency_*: å¤–å¹£äº¤æ˜“çµ±è¨ˆ
           - cross_bank_*: è·¨è¡Œäº¤æ˜“çµ±è¨ˆ
        
        6. é¢¨éšªæŒ‡æ¨™ (14 å€‹):
           - large_txn_*: å¤§é¡äº¤æ˜“çµ±è¨ˆï¼ˆP90 ä»¥ä¸Šï¼‰
           - small_txn_*: å°é¡äº¤æ˜“çµ±è¨ˆï¼ˆP10 ä»¥ä¸‹ï¼‰
           - out_concentration: è½‰å‡ºé›†ä¸­åº¦
           - in_concentration: è½‰å…¥é›†ä¸­åº¦
           - hour_entropy: äº¤æ˜“æ™‚æ®µç†µå€¼
        
        Args:
            out_txns (pd.DataFrame): è©²å¸³æˆ¶çš„è½‰å‡ºäº¤æ˜“è³‡æ–™ã€‚
            in_txns (pd.DataFrame): è©²å¸³æˆ¶çš„è½‰å…¥äº¤æ˜“è³‡æ–™ã€‚
            all_txns (pd.DataFrame): è©²å¸³æˆ¶çš„æ‰€æœ‰äº¤æ˜“è³‡æ–™ã€‚
        
        Returns:
            dict: åŒ…å« 62 å€‹ç‰¹å¾µçš„å­—å…¸ï¼Œkey ç‚ºç‰¹å¾µåç¨±ï¼Œvalue ç‚ºç‰¹å¾µå€¼ã€‚
        
        Note:
            - å°æ–¼ç„¡è½‰å‡ºæˆ–è½‰å…¥äº¤æ˜“çš„æƒ…æ³ï¼Œç›¸é—œç‰¹å¾µå¡«å……ç‚º 0
            - åˆ†æ¯å¯èƒ½ç‚º 0 çš„è¨ˆç®—æœƒåŠ ä¸Š 1e-6 é¿å…é™¤é›¶éŒ¯èª¤
        """
        features = {}
        
        # ===== 1. åŸºæœ¬äº¤æ˜“çµ±è¨ˆ =====
        features['out_count'] = len(out_txns)
        features['in_count'] = len(in_txns)
        features['total_count'] = len(all_txns)
        features['txn_ratio'] = features['out_count'] / max(features['in_count'], 1)
        features['in_out_diff'] = abs(features['out_count'] - features['in_count'])
        
        # ===== 2. é‡‘é¡ç‰¹å¾µ =====
        if len(out_txns) > 0:
            features['out_amt_sum'] = out_txns['txn_amt'].sum()
            features['out_amt_mean'] = out_txns['txn_amt'].mean()
            features['out_amt_std'] = out_txns['txn_amt'].std()
            features['out_amt_max'] = out_txns['txn_amt'].max()
            features['out_amt_min'] = out_txns['txn_amt'].min()
            features['out_amt_median'] = out_txns['txn_amt'].median()
            features['out_amt_cv'] = features['out_amt_std'] / (features['out_amt_mean'] + 1e-6)
            features['out_amt_q25'] = out_txns['txn_amt'].quantile(0.25)
            features['out_amt_q75'] = out_txns['txn_amt'].quantile(0.75)
            features['out_amt_iqr'] = features['out_amt_q75'] - features['out_amt_q25']
        else:
            for k in ['sum','mean','std','max','min','median','cv','q25','q75','iqr']:
                features[f'out_amt_{k}'] = 0
        
        if len(in_txns) > 0:
            features['in_amt_sum'] = in_txns['txn_amt'].sum()
            features['in_amt_mean'] = in_txns['txn_amt'].mean()
            features['in_amt_median'] = in_txns['txn_amt'].median()
        else:
            for k in ['sum','mean','median']:
                features[f'in_amt_{k}'] = 0
        
        features['amt_in_out_ratio'] = features['in_amt_sum'] / max(features['out_amt_sum'], 1)
        features['amt_balance'] = features['in_amt_sum'] - features['out_amt_sum']
        
        # ===== 3. æ™‚é–“ç‰¹å¾µ =====
        features['date_range'] = all_txns['txn_date'].max() - all_txns['txn_date'].min() + 1
        features['txn_velocity'] = features['total_count'] / features['date_range']
        features['active_days'] = all_txns['txn_date'].nunique()
        features['active_day_ratio'] = features['active_days'] / features['date_range']
        
        features['night_txn_count'] = len(all_txns[(all_txns['txn_hour'] < 6) | (all_txns['txn_hour'] >= 22)])
        features['night_txn_ratio'] = features['night_txn_count'] / features['total_count']
        features['work_hour_count'] = len(all_txns[(all_txns['txn_hour'] >= 9) & (all_txns['txn_hour'] < 18)])
        features['work_hour_ratio'] = features['work_hour_count'] / features['total_count']
        
        min_date = all_txns['txn_date'].min()
        max_date = all_txns['txn_date'].max()
        first_week = all_txns[all_txns['txn_date'] <= min_date + 7]
        last_week = all_txns[all_txns['txn_date'] >= max_date - 7]
        features['first_week_ratio'] = len(first_week) / features['total_count']
        features['last_week_ratio'] = len(last_week) / features['total_count']
        features['first_last_week_diff'] = abs(features['first_week_ratio'] - features['last_week_ratio'])
        
        # ===== 4. å°æ‰‹æ–¹ç‰¹å¾µ =====
        features['unique_from'] = in_txns['from_acct'].nunique()
        features['unique_to'] = out_txns['to_acct'].nunique()
        features['unique_counterparties'] = features['unique_from'] + features['unique_to']
        features['counterparty_diversity'] = features['unique_counterparties'] / max(features['total_count'], 1)
        
        # ===== 5. äº¤æ˜“é¡å‹ç‰¹å¾µ =====
        features['self_txn_count'] = len(all_txns[all_txns['is_self_txn'] == 'Y'])
        features['self_txn_ratio'] = features['self_txn_count'] / features['total_count']
        
        features['unique_channels'] = all_txns['channel_type'].nunique()
        features['channel_diversity'] = features['unique_channels'] / features['total_count']
        
        features['foreign_currency_count'] = len(all_txns[all_txns['currency_type'] != 'TWD'])
        features['foreign_currency_ratio'] = features['foreign_currency_count'] / features['total_count']
        features['unique_currencies'] = all_txns['currency_type'].nunique()
        
        features['cross_bank_out'] = len(out_txns[out_txns['to_acct_type'] == 2])
        features['cross_bank_in'] = len(in_txns[in_txns['from_acct_type'] == 2])
        features['cross_bank_total'] = features['cross_bank_out'] + features['cross_bank_in']
        features['cross_bank_ratio'] = features['cross_bank_total'] / features['total_count']
        
        # ===== 6. é¢¨éšªæŒ‡æ¨™ =====
        if len(out_txns) > 0:
            threshold_90 = out_txns['txn_amt'].quantile(0.9)
            large_txns = out_txns[out_txns['txn_amt'] > threshold_90]
            features['large_txn_count'] = len(large_txns)
            features['large_txn_ratio'] = features['large_txn_count'] / len(out_txns)
            features['large_txn_amt_sum'] = large_txns['txn_amt'].sum()
            features['large_txn_amt_ratio'] = features['large_txn_amt_sum'] / features['out_amt_sum']
        else:
            for k in ['count','ratio','amt_sum','amt_ratio']:
                features[f'large_txn_{k}'] = 0
        
        if len(out_txns) > 0:
            threshold_10 = out_txns['txn_amt'].quantile(0.1)
            features['small_txn_count'] = len(out_txns[out_txns['txn_amt'] < threshold_10])
            features['small_txn_ratio'] = features['small_txn_count'] / len(out_txns)
        else:
            features['small_txn_count'] = 0
            features['small_txn_ratio'] = 0
        
        if features['out_count'] > 0:
            to_counts = out_txns['to_acct'].value_counts()
            features['out_concentration'] = to_counts.iloc[0] / features['out_count']
            features['out_top3_concentration'] = to_counts.head(3).sum() / features['out_count']
        else:
            features['out_concentration'] = 0
            features['out_top3_concentration'] = 0
        
        if features['in_count'] > 0:
            from_counts = in_txns['from_acct'].value_counts()
            features['in_concentration'] = from_counts.iloc[0] / features['in_count']
        else:
            features['in_concentration'] = 0
        
        daily_counts = all_txns.groupby('txn_date').size()
        if len(daily_counts) > 0:
            features['max_daily_txns'] = daily_counts.max()
            features['avg_daily_txns'] = daily_counts.mean()
            features['std_daily_txns'] = daily_counts.std()
            features['burst_score'] = features['max_daily_txns'] / (features['avg_daily_txns'] + 1e-6)
        else:
            for k in ['max_daily_txns','avg_daily_txns','std_daily_txns','burst_score']:
                features[k] = 0
        
        hour_dist = all_txns['txn_hour'].value_counts(normalize=True)
        features['hour_entropy'] = -np.sum(hour_dist * np.log(hour_dist + 1e-10))
        
        features['unique_dates'] = all_txns['txn_date'].nunique()
        features['date_coverage'] = features['unique_dates'] / features['date_range']
        
        return features
    
    def _get_empty_features(self):
        """è¿”å›ç©ºç‰¹å¾µå­—å…¸ï¼ˆç”¨æ–¼ç„¡äº¤æ˜“å¸³æˆ¶ï¼‰ã€‚
        
        ç•¶å¸³æˆ¶æ²’æœ‰ä»»ä½•äº¤æ˜“è¨˜éŒ„æ™‚ï¼Œå›å‚³æ‰€æœ‰ç‰¹å¾µå€¼ç‚º 0 çš„å­—å…¸ã€‚
        ç¢ºä¿ç‰¹å¾µç¶­åº¦ä¸€è‡´æ€§ï¼Œé¿å…å¾ŒçºŒæ¨¡å‹è¨“ç·´å‡ºéŒ¯ã€‚
        
        Returns:
            dict: åŒ…å« 62 å€‹ç‰¹å¾µçš„å­—å…¸ï¼Œæ‰€æœ‰å€¼å‡ç‚º 0ã€‚
        
        Note:
            æ­¤æ–¹æ³•åˆ—å‡ºæ‰€æœ‰ç‰¹å¾µåç¨±ï¼Œç”¨æ–¼ï¼š
            1. è™•ç†ç„¡äº¤æ˜“å¸³æˆ¶çš„é›¶å¡«å……
            2. ä½œç‚ºå®Œæ•´ç‰¹å¾µæ¸…å–®çš„åƒè€ƒæ–‡ä»¶
        """
        return {k: 0 for k in [
            'out_count','in_count','total_count','txn_ratio','in_out_diff',
            'out_amt_sum','out_amt_mean','out_amt_std','out_amt_max','out_amt_min','out_amt_median',
            'out_amt_cv','out_amt_q25','out_amt_q75','out_amt_iqr',
            'in_amt_sum','in_amt_mean','in_amt_median','amt_in_out_ratio','amt_balance',
            'date_range','txn_velocity','active_days','active_day_ratio',
            'night_txn_count','night_txn_ratio','work_hour_count','work_hour_ratio',
            'first_week_ratio','last_week_ratio','first_last_week_diff',
            'unique_from','unique_to','unique_counterparties','counterparty_diversity',
            'self_txn_count','self_txn_ratio','unique_channels','channel_diversity',
            'foreign_currency_count','foreign_currency_ratio','unique_currencies',
            'cross_bank_out','cross_bank_in','cross_bank_total','cross_bank_ratio',
            'large_txn_count','large_txn_ratio','large_txn_amt_sum','large_txn_amt_ratio',
            'small_txn_count','small_txn_ratio','out_concentration','out_top3_concentration',
            'in_concentration','max_daily_txns','avg_daily_txns','std_daily_txns',
            'burst_score','hour_entropy','unique_dates','date_coverage'
        ]}
